{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from models.gat_encoder import GATEncoder\n",
    "from Dataset.gat_dataset import GATDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Code for GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset and dataloader\n",
    "data_path = \"Dataset\\processed.jsonl\"  # Replace with your dataset path\n",
    "dataset = GATDataset(data_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GATEncoder(\n",
       "  (gat1): GATConv(768, 64, heads=4)\n",
       "  (gat2): GATConv(256, 32, heads=1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the GAT model\n",
    "gat_model = GATEncoder(in_channels=768, hidden_channels=64, out_channels=32, heads=4)\n",
    "gat_model.train()  # Set model to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy objective (MSE loss)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(gat_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.21608763337135314\n",
      "Epoch 2/100, Loss: 0.18426741063594818\n",
      "Epoch 3/100, Loss: 0.13559029027819633\n",
      "Epoch 4/100, Loss: 0.1079394280910492\n",
      "Epoch 5/100, Loss: 0.1065760925412178\n",
      "Epoch 6/100, Loss: 0.0951688177883625\n",
      "Epoch 7/100, Loss: 0.09486806392669678\n",
      "Epoch 8/100, Loss: 0.09466254562139512\n",
      "Epoch 9/100, Loss: 0.09888159185647964\n",
      "Epoch 10/100, Loss: 0.09477415382862091\n",
      "Epoch 11/100, Loss: 0.09652042835950851\n",
      "Epoch 12/100, Loss: 0.09184089899063111\n",
      "Epoch 13/100, Loss: 0.0952316477894783\n",
      "Epoch 14/100, Loss: 0.09304699525237084\n",
      "Epoch 15/100, Loss: 0.09469532817602158\n",
      "Epoch 16/100, Loss: 0.09131534323096276\n",
      "Epoch 17/100, Loss: 0.0945090651512146\n",
      "Epoch 18/100, Loss: 0.09299236536026001\n",
      "Epoch 19/100, Loss: 0.09167257621884346\n",
      "Epoch 20/100, Loss: 0.090387711673975\n",
      "Epoch 21/100, Loss: 0.09034134224057197\n",
      "Epoch 22/100, Loss: 0.09319190531969071\n",
      "Epoch 23/100, Loss: 0.08991318494081497\n",
      "Epoch 24/100, Loss: 0.09075582921504974\n",
      "Epoch 25/100, Loss: 0.0924159824848175\n",
      "Epoch 26/100, Loss: 0.0906685896217823\n",
      "Epoch 27/100, Loss: 0.08713940307497978\n",
      "Epoch 28/100, Loss: 0.09402678310871124\n",
      "Epoch 29/100, Loss: 0.08843089267611504\n",
      "Epoch 30/100, Loss: 0.09116027280688285\n",
      "Epoch 31/100, Loss: 0.09068273231387139\n",
      "Epoch 32/100, Loss: 0.09398326575756073\n",
      "Epoch 33/100, Loss: 0.08990327417850494\n",
      "Epoch 34/100, Loss: 0.09097363278269768\n",
      "Epoch 35/100, Loss: 0.08875250369310379\n",
      "Epoch 36/100, Loss: 0.08717467710375786\n",
      "Epoch 37/100, Loss: 0.0918058305978775\n",
      "Epoch 38/100, Loss: 0.09157742485404015\n",
      "Epoch 39/100, Loss: 0.09431119337677955\n",
      "Epoch 40/100, Loss: 0.0929063193500042\n",
      "Epoch 41/100, Loss: 0.09081452265381813\n",
      "Epoch 42/100, Loss: 0.09086100608110428\n",
      "Epoch 43/100, Loss: 0.0905029334127903\n",
      "Epoch 44/100, Loss: 0.08776016607880592\n",
      "Epoch 45/100, Loss: 0.08911887928843498\n",
      "Epoch 46/100, Loss: 0.088583355396986\n",
      "Epoch 47/100, Loss: 0.08877399712800979\n",
      "Epoch 48/100, Loss: 0.08588684648275376\n",
      "Epoch 49/100, Loss: 0.09123606458306313\n",
      "Epoch 50/100, Loss: 0.08584362864494324\n",
      "Epoch 51/100, Loss: 0.08968271613121033\n",
      "Epoch 52/100, Loss: 0.09116761088371277\n",
      "Epoch 53/100, Loss: 0.08560270294547082\n",
      "Epoch 54/100, Loss: 0.08782808333635331\n",
      "Epoch 55/100, Loss: 0.0866782821714878\n",
      "Epoch 56/100, Loss: 0.08418397083878518\n",
      "Epoch 57/100, Loss: 0.08769289180636405\n",
      "Epoch 58/100, Loss: 0.08541573584079742\n",
      "Epoch 59/100, Loss: 0.0889336682856083\n",
      "Epoch 60/100, Loss: 0.09052253738045693\n",
      "Epoch 61/100, Loss: 0.08503644391894341\n",
      "Epoch 62/100, Loss: 0.0883705586194992\n",
      "Epoch 63/100, Loss: 0.09218232408165931\n",
      "Epoch 64/100, Loss: 0.08489515259861946\n",
      "Epoch 65/100, Loss: 0.08912633210420609\n",
      "Epoch 66/100, Loss: 0.08433945253491401\n",
      "Epoch 67/100, Loss: 0.0898413561284542\n",
      "Epoch 68/100, Loss: 0.09192591980099678\n",
      "Epoch 69/100, Loss: 0.08914792835712433\n",
      "Epoch 70/100, Loss: 0.08712768405675889\n",
      "Epoch 71/100, Loss: 0.086761973798275\n",
      "Epoch 72/100, Loss: 0.08580818474292755\n",
      "Epoch 73/100, Loss: 0.08823200315237045\n",
      "Epoch 74/100, Loss: 0.08778372183442115\n",
      "Epoch 75/100, Loss: 0.08901937454938888\n",
      "Epoch 76/100, Loss: 0.08931776508688927\n",
      "Epoch 77/100, Loss: 0.0910433903336525\n",
      "Epoch 78/100, Loss: 0.09053004533052444\n",
      "Epoch 79/100, Loss: 0.08364758267998695\n",
      "Epoch 80/100, Loss: 0.08931127414107323\n",
      "Epoch 81/100, Loss: 0.08384551331400872\n",
      "Epoch 82/100, Loss: 0.08859786689281464\n",
      "Epoch 83/100, Loss: 0.08888334110379219\n",
      "Epoch 84/100, Loss: 0.08834701031446457\n",
      "Epoch 85/100, Loss: 0.08747418969869614\n",
      "Epoch 86/100, Loss: 0.08540500700473785\n",
      "Epoch 87/100, Loss: 0.08820619583129882\n",
      "Epoch 88/100, Loss: 0.0914412945508957\n",
      "Epoch 89/100, Loss: 0.08580302372574806\n",
      "Epoch 90/100, Loss: 0.08553601428866386\n",
      "Epoch 91/100, Loss: 0.09148939549922944\n",
      "Epoch 92/100, Loss: 0.08452377170324325\n",
      "Epoch 93/100, Loss: 0.084483952075243\n",
      "Epoch 94/100, Loss: 0.0918968603014946\n",
      "Epoch 95/100, Loss: 0.08304093256592751\n",
      "Epoch 96/100, Loss: 0.08653188794851303\n",
      "Epoch 97/100, Loss: 0.08943578153848648\n",
      "Epoch 98/100, Loss: 0.08429448679089546\n",
      "Epoch 99/100, Loss: 0.08846790716052055\n",
      "Epoch 100/100, Loss: 0.08796036541461945\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Extract data from the batch\n",
    "        node_features = batch[\"node_features\"].squeeze(0)  # [num_nodes, in_channels]\n",
    "        edge_list = batch[\"edge_list\"].squeeze(0)  # [2, num_edges]\n",
    "        \n",
    "        # Forward pass\n",
    "        output_embeddings = gat_model(node_features, edge_list)  # [num_nodes, out_channels]\n",
    "        \n",
    "        # Generate a dummy target (same shape as output)\n",
    "        target_embeddings = torch.rand_like(output_embeddings)  # Random target embeddings\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output_embeddings, target_embeddings)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Code for Triple List Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from models.triple_list_encoder import TripleListEncoder \n",
    "from Dataset.triple_list_dataset import TripleListDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "data_path = \"Dataset\\processed.jsonl\"  # Replace with the path to your preprocessed dataset\n",
    "dataset = TripleListDataset(data_path)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TripleListEncoder(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (projection_layer): Linear(in_features=768, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize TripleListEncoder\n",
    "embedding_dim = 128\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DataPrep/tokenizer_w_special_tokens\")\n",
    "encoder = TripleListEncoder(\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    embedding_dim=128,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "encoder.train()  # Set to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy optimizer\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.21631427407264708\n",
      "Epoch 2/3, Loss: 0.4535333514213562\n",
      "Epoch 3/3, Loss: 0.11575669646263123\n"
     ]
    }
   ],
   "source": [
    "# Dummy training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Extract batch data\n",
    "        triple_input_ids = batch[\"triple_input_ids\"]  # [batch_size, max_seq_length]\n",
    "        triple_attention_mask = batch[\"triple_attention_mask\"]  # [batch_size, max_seq_length]\n",
    "\n",
    "        # Forward pass through TripleListEncoder\n",
    "        triple_embeddings = encoder(triple_input_ids, triple_attention_mask)  # [batch_size, embedding_dim]\n",
    "\n",
    "        # Generate dummy target embeddings\n",
    "        target_embeddings = torch.rand_like(triple_embeddings)  # [batch_size, embedding_dim]\n",
    "\n",
    "        # Compute a dummy loss (MSE loss)\n",
    "        loss = torch.nn.functional.mse_loss(triple_embeddings, target_embeddings)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Code for Text Unicoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from models.text_encoder import TextUnicoder\n",
    "from Dataset.text_dataset import TextDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "text_dataset = TextDataset(\"Dataset\\processed.jsonl\")\n",
    "text_dataloader = DataLoader(text_dataset, batch_size=2, shuffle=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DataPrep/tokenizer_w_special_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextUnicoder(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (projection_layer): Linear(in_features=768, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Text Unicoder\n",
    "text_unicoder = TextUnicoder(\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    embedding_dim=128,\n",
    "    tokenizer=tokenizer  # Reuse the tokenizer with added special tokens\n",
    ")\n",
    "text_unicoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy optimizer\n",
    "optimizer = torch.optim.Adam(text_unicoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.4590041220188141\n",
      "Epoch 2/3, Loss: 0.16240791082382203\n",
      "Epoch 3/3, Loss: 0.12611118704080582\n"
     ]
    }
   ],
   "source": [
    "# Dummy training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in text_dataloader:\n",
    "        text_input_ids = batch[\"text_input_ids\"]\n",
    "        text_attention_mask = batch[\"text_attention_mask\"]\n",
    "\n",
    "        # Forward pass through Text Unicoder\n",
    "        text_embeddings = text_unicoder(text_input_ids, text_attention_mask)\n",
    "\n",
    "        # Dummy target embeddings\n",
    "        target_embeddings = torch.rand_like(text_embeddings)\n",
    "\n",
    "        # Compute a dummy loss (e.g., MSE)\n",
    "        loss = torch.nn.functional.mse_loss(text_embeddings, target_embeddings)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(text_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
