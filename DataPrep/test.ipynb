{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = r\"C:\\Users\\Ribhav\\Desktop\\UIUC\\Classes\\Git Repos\\Work\\BGTT\\Dataset\\WikiOFGraph-test.jsonl\"\n",
    "file_path=r\"..\\Dataset\\WikiOFGraph-test.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5 new tokens: ['[TRIPLE_START]', '[TRIPLE_END]', '[SUBJECT]', '[PREDICATE]', '[OBJECT]']\n",
      "Input IDs: tensor([[ 101, 7592, 2088, 1010, 2023, 2003, 1037, 2460, 6251, 1012,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Decoded back: ['[CLS] hello world, this is a short sentence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1. Define special tokens\n",
    "# ------------------------------------------------------------------------------\n",
    "special_tokens = [\n",
    "    \"[TRIPLE_START]\",\n",
    "    \"[TRIPLE_END]\",\n",
    "    \"[SUBJECT]\",\n",
    "    \"[PREDICATE]\",\n",
    "    \"[OBJECT]\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Initialize tokenizer and add special tokens\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize tokenizer (fast or python version)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Add custom tokens\n",
    "num_added_toks = tokenizer.add_tokens(special_tokens)\n",
    "print(f\"Added {num_added_toks} new tokens:\", special_tokens)\n",
    "\n",
    "# Example max_length for both text and triple encodings\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "\n",
    "\n",
    "# --------- Test Example ----------\n",
    "\n",
    "# Example text\n",
    "text_string = \"Hello world, this is a short sentence.\"\n",
    "\n",
    "# Encode (or tokenize) with padding and truncation\n",
    "encoding = tokenizer(\n",
    "    text_string,\n",
    "    padding=\"max_length\",  # can also use 'longest' if batching multiple sequences\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\"    # return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encoding[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encoding[\"attention_mask\"])\n",
    "print(\"Decoded back:\", tokenizer.batch_decode(encoding[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_triples(triple_str):\n",
    "    \"\"\"\n",
    "    Given a string like:\n",
    "      '(<S> College of william & mary| <P> Represented by| <O> 1977 william & mary indians football team), (<S> ... ), ...'\n",
    "    Return a list of dictionaries with keys 'subject', 'predicate', 'object'.\n",
    "    \"\"\"\n",
    "    # Split on '),'\n",
    "    triple_str = triple_str.strip()\n",
    "    raw_triples = triple_str.split('),')\n",
    "    \n",
    "    # Clean up each triple chunk\n",
    "    # We expect them to be in format: \n",
    "    # '(<S> College of william & mary| <P> Represented by| <O> 1977 william & mary indians football team)'\n",
    "    parsed_triples = []\n",
    "    for raw_triple in raw_triples:\n",
    "        # Remove possible surrounding parentheses and spaces\n",
    "        raw_triple = raw_triple.strip()\n",
    "        # e.g., '(<S> College of william & mary| <P> Represented by| <O> 1977 william & mary indians football team)'\n",
    "        raw_triple = raw_triple.replace('(', '').replace(')', '').strip()\n",
    "        \n",
    "        # Now split on '|'\n",
    "        parts = raw_triple.split('|')\n",
    "        # parts[0] should have '<S> ...', parts[1] => '<P> ...', parts[2] => '<O> ...'\n",
    "        subject = parts[0].replace('<S>', '').strip()\n",
    "        predicate = parts[1].replace('<P>', '').strip()\n",
    "        obj = parts[2].replace('<O>', '').strip()\n",
    "        \n",
    "        parsed_triples.append({\n",
    "            'subject': subject,\n",
    "            'predicate': predicate,\n",
    "            'object': obj\n",
    "        })\n",
    "    \n",
    "    return parsed_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_triples(triples):\n",
    "    \"\"\"\n",
    "    Convert the set of triples into a linear sequence like:\n",
    "      [TRIPLE_START] [SUBJECT] Bob [PREDICATE] likes [OBJECT] apples [TRIPLE_END] ...\n",
    "    \"\"\"\n",
    "    triple_str_list = []\n",
    "    for t in triples:\n",
    "        triple_repr = (\n",
    "            \"[TRIPLE_START] [SUBJECT] \" + t['subject'] +\n",
    "            \" [PREDICATE] \" + t['predicate'] +\n",
    "            \" [OBJECT] \" + t['object'] +\n",
    "            \" [TRIPLE_END]\"\n",
    "        )\n",
    "        triple_str_list.append(triple_repr)\n",
    "    \n",
    "    # Concat all\n",
    "    return \" \".join(triple_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_edge_list(triples):\n",
    "#     \"\"\"\n",
    "#     Build an edge list from the triples:\n",
    "#     E.g. [(\"College of william & mary\", \"1977 william & mary indians football team\"),\n",
    "#           (\"1977 william & mary indians football team\", \"1977 ncaa division i football season\"),\n",
    "#           ... ]\n",
    "#     \"\"\"\n",
    "#     edges = []\n",
    "#     for t in triples:\n",
    "#         edges.append((t['subject'], t['object']))\n",
    "#     return edges\n",
    "\n",
    "def build_edge_list(triples):\n",
    "    \"\"\"\n",
    "    Build an edge list and encode edge types:\n",
    "    E.g. [(\"College of william & mary\", \"Represented by\", \"1977 william & mary indians football team\"),\n",
    "          (\"1977 william & mary indians football team\", \"Played in\", \"1977 ncaa division i football season\"),\n",
    "          ...]\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    edge_types = []\n",
    "    for t in triples:\n",
    "        edges.append((t['subject'], t['object']))\n",
    "        edge_types.append(t['predicate'])  # Include predicate (relationship)\n",
    "    return edges, edge_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency_matrix(triples):\n",
    "    \"\"\"\n",
    "    Build an adjacency matrix from the given triples.\n",
    "    1. Collect all unique entities.\n",
    "    2. Create a matrix of size NxN (N = number of unique entities).\n",
    "    3. Fill 1 where there's an edge from subject to object (can also store\n",
    "       relationship/predicate if you want a more complex adjacency).\n",
    "    \"\"\"\n",
    "    # Collect unique entities\n",
    "    entities = set()\n",
    "    for t in triples:\n",
    "        entities.add(t['subject'])\n",
    "        entities.add(t['object'])\n",
    "    entities = list(entities)  # fix an ordering\n",
    "    \n",
    "    # Create a map from entity -> index\n",
    "    entity2idx = {ent: i for i, ent in enumerate(entities)}\n",
    "    \n",
    "    # Initialize adjacency matrix\n",
    "    N = len(entities)\n",
    "    adjacency_matrix = np.zeros((N, N), dtype=int)\n",
    "    \n",
    "    # Fill adjacency matrix\n",
    "    for t in triples:\n",
    "        s_idx = entity2idx[t['subject']]\n",
    "        o_idx = entity2idx[t['object']]\n",
    "        adjacency_matrix[s_idx, o_idx] = 1  # or any weighting if needed\n",
    "    \n",
    "    return adjacency_matrix, entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "max_test = 5  # Process only the first 5 entries as a test\n",
    "\n",
    "processed_dataset = []\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= max_test:\n",
    "            break  # Stop after processing max_test entries\n",
    "\n",
    "        entry = json.loads(line)\n",
    "        triple_string = entry['triplet']\n",
    "        text_string = entry['text']\n",
    "\n",
    "        # a) Parse triples\n",
    "        parsed = parse_triples(triple_string)\n",
    "\n",
    "        # b) Linearize the triple set\n",
    "        linearized_triples = linearize_triples(parsed)\n",
    "\n",
    "        # c) Tokenize the text & linearized triples (with padding & truncation)\n",
    "        # ---------------------------------------------------------\n",
    "        text_encodings = tokenizer(\n",
    "            text_string,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        triple_encodings = tokenizer(\n",
    "            linearized_triples,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Convert input_ids and attention_masks to Python lists\n",
    "        text_input_ids = text_encodings[\"input_ids\"].squeeze(0).tolist()\n",
    "        text_attention_mask = text_encodings[\"attention_mask\"].squeeze(0).tolist()\n",
    "\n",
    "        triple_input_ids = triple_encodings[\"input_ids\"].squeeze(0).tolist()\n",
    "        triple_attention_mask = triple_encodings[\"attention_mask\"].squeeze(0).tolist()\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        # d) Graph structure encoding\n",
    "        edge_list, edge_types = build_edge_list(parsed)  # Modified to include edge types\n",
    "        adjacency_matrix, entities = build_adjacency_matrix(parsed)\n",
    "\n",
    "        # e) Generate node features using BERT embeddings\n",
    "        entity_embeddings = []\n",
    "        for entity in entities:\n",
    "            inputs = bert_tokenizer(entity, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            outputs = bert_model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over tokens\n",
    "            entity_embeddings.append(embedding.squeeze(0).tolist())\n",
    "\n",
    "        # f) Store final outputs\n",
    "        processed_dataset.append({\n",
    "            \"original_text\": text_string,\n",
    "            \"original_triples\": triple_string,\n",
    "            \"parsed_triples\": parsed,\n",
    "            \"linearized_triples_str\": linearized_triples,\n",
    "\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            \"text_attention_mask\": text_attention_mask,\n",
    "            \"triple_input_ids\": triple_input_ids,\n",
    "            \"triple_attention_mask\": triple_attention_mask,\n",
    "\n",
    "            \"edge_list\": edge_list,  # Edge list without relationships\n",
    "            \"edge_types\": edge_types,  # Encoded relationships\n",
    "            \"node_features\": entity_embeddings,  # Precomputed entity embeddings\n",
    "            \"entities\": entities  # Original entity list\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 entries and saved to '..\\Dataset\\processed.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "out_file = r\"..\\Dataset\\processed.jsonl\"\n",
    "with open(out_file, 'w', encoding='utf-8') as f_out:\n",
    "    for item in processed_dataset:\n",
    "        f_out.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Processed {len(processed_dataset)} entries and saved to '{out_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 50 records\n",
      "Train: 40 records\n",
      "Val:   5 records\n",
      "Test:  5 records\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Final processed dataset is in a Python list called 'processed_dataset'\n",
    "data = processed_dataset[:]  # make a copy if you'd like\n",
    "\n",
    "# Shuffle the data in-place so we get a random mix\n",
    "random.shuffle(data)\n",
    "\n",
    "# Decide splits\n",
    "train_ratio = 0.80  # 80% for training\n",
    "val_ratio = 0.10    # 10% for validation\n",
    "test_ratio = 0.10   # 10% for testing\n",
    "\n",
    "data_size = len(data)\n",
    "train_end = int(train_ratio * data_size)\n",
    "val_end = train_end + int(val_ratio * data_size)\n",
    "\n",
    "train_data = data[:train_end]\n",
    "val_data = data[train_end:val_end]\n",
    "test_data = data[val_end:]\n",
    "\n",
    "print(f\"Total: {data_size} records\")\n",
    "print(f\"Train: {len(train_data)} records\")\n",
    "print(f\"Val:   {len(val_data)} records\")\n",
    "print(f\"Test:  {len(test_data)} records\")\n",
    "\n",
    "def save_jsonl(data_list, filename):\n",
    "    \"\"\"\n",
    "    Save a list of dict records to a .jsonl file (one JSON record per line).\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for record in data_list:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "save_jsonl(train_data, r\"..\\FinalData\\train_data.jsonl\")\n",
    "save_jsonl(val_data,   r\"..\\FinalData\\val_data.jsonl\")\n",
    "save_jsonl(test_data,  r\"..\\FinalData\\test_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedGraphTextDataset(Dataset):\n",
    "    def __init__(self, processed_dataset):\n",
    "        \"\"\"\n",
    "        processed_dataset is assumed to be a list of dicts, something like:\n",
    "          {\n",
    "            'linearized_triples_str': ...\n",
    "            'text_input_ids': ...\n",
    "            'text_attention_mask': ...\n",
    "            'triple_input_ids': ...\n",
    "            'triple_attention_mask': ...\n",
    "            'edge_list': ...\n",
    "            'adjacency_matrix': ...\n",
    "            ...\n",
    "          }\n",
    "        We'll create 2x many samples:\n",
    "          - 1 Graph→Text sample\n",
    "          - 1 Text→Graph sample\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        for entry in processed_dataset:\n",
    "            # 1) Graph→Text\n",
    "            self.samples.append({\n",
    "                \"task\": \"G2T\",  # Graph→Text\n",
    "                \"input_ids\": entry[\"triple_input_ids\"],      # model input is triple tokens\n",
    "                \"attention_mask\": entry[\"triple_attention_mask\"],\n",
    "                # We also store graph structure if needed\n",
    "                \"graph_adj\": entry.get(\"adjacency_matrix\", None),  # or edge_list\n",
    "                # The output/label we want is the text token IDs\n",
    "                \"label_ids\": entry[\"text_input_ids\"],\n",
    "                \n",
    "                # Possibly store the text attention mask as a \"label mask\" \n",
    "                # if you need it for language modeling or such.\n",
    "                \"label_mask\": entry[\"text_attention_mask\"],\n",
    "            })\n",
    "\n",
    "            # 2) Text→Graph\n",
    "            self.samples.append({\n",
    "                \"task\": \"T2G\",  # Text→Graph\n",
    "                \"input_ids\": entry[\"text_input_ids\"],        # model input is text tokens\n",
    "                \"attention_mask\": entry[\"text_attention_mask\"],\n",
    "                # For output, we want the triple tokens\n",
    "                \"label_ids\": entry[\"triple_input_ids\"],\n",
    "                \n",
    "                # Potentially, you might want to generate the adjacency matrix too,\n",
    "                # but that often goes beyond simple token generation.\n",
    "                \"graph_adj\": entry.get(\"adjacency_matrix\", None),\n",
    "                \n",
    "                # If you do some sequence modeling for the triple tokens, store mask\n",
    "                \"label_mask\": entry[\"triple_attention_mask\"],\n",
    "            })\n",
    "        \n",
    "        # Shuffle samples so we don’t get all G2T then T2G in order\n",
    "        # (Though you can also rely on DataLoader shuffle.)\n",
    "        # import random\n",
    "        # random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_collate_fn(batch_list):\n",
    "    \"\"\"\n",
    "    Example collate function that merges a list of samples into a batch.\n",
    "    The 'batch_list' is a list of dictionaries from __getitem__().\n",
    "    We'll produce batched tensors for input_ids, label_ids, etc.\n",
    "    We also keep the 'task' so the model knows which forward pass to apply.\n",
    "    \"\"\"\n",
    "    # Extract each field\n",
    "    tasks = [item[\"task\"] for item in batch_list]\n",
    "    \n",
    "    input_ids = [item[\"input_ids\"] for item in batch_list]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch_list]\n",
    "    label_ids = [item[\"label_ids\"] for item in batch_list]\n",
    "    label_masks = [item[\"label_mask\"] for item in batch_list]\n",
    "    graph_adjs = [item[\"graph_adj\"] for item in batch_list]  # or None\n",
    "    \n",
    "    # Convert to tensors (assuming everything is a list of int)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "    label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "    label_masks = torch.tensor(label_masks, dtype=torch.long)\n",
    "    \n",
    "    # graph_adjs might remain a list of adjacency matrices (each NxN), \n",
    "    # or you can turn them into a 3D tensor if they're all the same size.\n",
    "    \n",
    "    batch = {\n",
    "        \"task\": tasks,  # list of tasks in this batch\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"label_ids\": label_ids,\n",
    "        \"label_mask\": label_masks,\n",
    "        \"graph_adj\": graph_adjs\n",
    "    }\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Dict Keys: dict_keys(['task', 'input_ids', 'attention_mask', 'label_ids', 'label_mask', 'graph_adj'])\n",
      "Tasks: ['G2T', 'G2T', 'T2G', 'T2G']\n",
      "Input IDs shape: torch.Size([4, 128])\n",
      "Label IDs shape: torch.Size([4, 128])\n",
      "[[[0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 1, 0, 0]], [[0, 0, 0, 0], [1, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [1, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0], [0, 0, 0], [1, 1, 0]]]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "# ------------------------------------------------------------------\n",
    "# Example usage\n",
    "# ------------------------------------------------------------------\n",
    "def get_mixed_dataloader(processed_dataset, batch_size=4, shuffle=True):\n",
    "    dataset = MixedGraphTextDataset(processed_dataset)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=mixed_collate_fn\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "# Suppose you have a processed_dataset from your earlier code:\n",
    "# processed_dataset = [...]\n",
    "# Now you can build a mixed dataloader:\n",
    "mixed_loader = get_mixed_dataloader(processed_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "for batch in mixed_loader:\n",
    "    print(\"Batch Dict Keys:\", batch.keys())\n",
    "    # batch is a dictionary with:\n",
    "    #   batch[\"task\"] -> e.g. [\"G2T\",\"T2G\",\"G2T\",\"T2G\"]\n",
    "    #   batch[\"input_ids\"] -> shape [B, seq_len]\n",
    "    #   batch[\"label_ids\"] -> shape [B, seq_len]\n",
    "    #   ...\n",
    "    # You can decide how to handle them in your training loop.\n",
    "    print(\"Tasks:\", batch[\"task\"])\n",
    "    print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Label IDs shape:\", batch[\"label_ids\"].shape)\n",
    "    print(batch[\"graph_adj\"])\n",
    "    # ... Forward pass ...\n",
    "    break  # just show the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
