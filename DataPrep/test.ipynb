{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Ribhav\\Desktop\\UIUC\\Classes\\Git Repos\\Work\\BGTT\\Dataset\\WikiOFGraph-test.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5 new tokens: ['[TRIPLE_START]', '[TRIPLE_END]', '[SUBJECT]', '[PREDICATE]', '[OBJECT]']\n",
      "Input IDs: tensor([[ 101, 7592, 2088, 1010, 2023, 2003, 1037, 2460, 6251, 1012,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Decoded back: ['[CLS] hello world, this is a short sentence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1. Define special tokens\n",
    "# ------------------------------------------------------------------------------\n",
    "special_tokens = [\n",
    "    \"[TRIPLE_START]\",\n",
    "    \"[TRIPLE_END]\",\n",
    "    \"[SUBJECT]\",\n",
    "    \"[PREDICATE]\",\n",
    "    \"[OBJECT]\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Initialize tokenizer and add special tokens\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize tokenizer (fast or python version)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Add custom tokens\n",
    "num_added_toks = tokenizer.add_tokens(special_tokens)\n",
    "print(f\"Added {num_added_toks} new tokens:\", special_tokens)\n",
    "\n",
    "# Example max_length for both text and triple encodings\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "\n",
    "\n",
    "# --------- Test Example ----------\n",
    "\n",
    "# Example text\n",
    "text_string = \"Hello world, this is a short sentence.\"\n",
    "\n",
    "# Encode (or tokenize) with padding and truncation\n",
    "encoding = tokenizer(\n",
    "    text_string,\n",
    "    padding=\"max_length\",  # can also use 'longest' if batching multiple sequences\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\"    # return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encoding[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encoding[\"attention_mask\"])\n",
    "print(\"Decoded back:\", tokenizer.batch_decode(encoding[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_triples(triple_str):\n",
    "    \"\"\"\n",
    "    Given a string like:\n",
    "      '(<S> College of william & mary| <P> Represented by| <O> 1977 william & mary indians football team), (<S> ... ), ...'\n",
    "    Return a list of dictionaries with keys 'subject', 'predicate', 'object'.\n",
    "    \"\"\"\n",
    "    # Split on '),'\n",
    "    triple_str = triple_str.strip()\n",
    "    raw_triples = triple_str.split('),')\n",
    "    \n",
    "    # Clean up each triple chunk\n",
    "    # We expect them to be in format: \n",
    "    # '(<S> College of william & mary| <P> Represented by| <O> 1977 william & mary indians football team)'\n",
    "    parsed_triples = []\n",
    "    for raw_triple in raw_triples:\n",
    "        # Remove possible surrounding parentheses and spaces\n",
    "        raw_triple = raw_triple.strip()\n",
    "        # e.g., '(<S> College of william & mary| <P> Represented by| <O> 1977 william & mary indians football team)'\n",
    "        raw_triple = raw_triple.replace('(', '').replace(')', '').strip()\n",
    "        \n",
    "        # Now split on '|'\n",
    "        parts = raw_triple.split('|')\n",
    "        # parts[0] should have '<S> ...', parts[1] => '<P> ...', parts[2] => '<O> ...'\n",
    "        subject = parts[0].replace('<S>', '').strip()\n",
    "        predicate = parts[1].replace('<P>', '').strip()\n",
    "        obj = parts[2].replace('<O>', '').strip()\n",
    "        \n",
    "        parsed_triples.append({\n",
    "            'subject': subject,\n",
    "            'predicate': predicate,\n",
    "            'object': obj\n",
    "        })\n",
    "    \n",
    "    return parsed_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_triples(triples):\n",
    "    \"\"\"\n",
    "    Convert the set of triples into a linear sequence like:\n",
    "      [TRIPLE_START] [SUBJECT] Bob [PREDICATE] likes [OBJECT] apples [TRIPLE_END] ...\n",
    "    \"\"\"\n",
    "    triple_str_list = []\n",
    "    for t in triples:\n",
    "        triple_repr = (\n",
    "            \"[TRIPLE_START] [SUBJECT] \" + t['subject'] +\n",
    "            \" [PREDICATE] \" + t['predicate'] +\n",
    "            \" [OBJECT] \" + t['object'] +\n",
    "            \" [TRIPLE_END]\"\n",
    "        )\n",
    "        triple_str_list.append(triple_repr)\n",
    "    \n",
    "    # Concat all\n",
    "    return \" \".join(triple_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_list(triples):\n",
    "    \"\"\"\n",
    "    Build an edge list from the triples:\n",
    "    E.g. [(\"College of william & mary\", \"1977 william & mary indians football team\"),\n",
    "          (\"1977 william & mary indians football team\", \"1977 ncaa division i football season\"),\n",
    "          ... ]\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    for t in triples:\n",
    "        edges.append((t['subject'], t['object']))\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency_matrix(triples):\n",
    "    \"\"\"\n",
    "    Build an adjacency matrix from the given triples.\n",
    "    1. Collect all unique entities.\n",
    "    2. Create a matrix of size NxN (N = number of unique entities).\n",
    "    3. Fill 1 where there's an edge from subject to object (can also store\n",
    "       relationship/predicate if you want a more complex adjacency).\n",
    "    \"\"\"\n",
    "    # Collect unique entities\n",
    "    entities = set()\n",
    "    for t in triples:\n",
    "        entities.add(t['subject'])\n",
    "        entities.add(t['object'])\n",
    "    entities = list(entities)  # fix an ordering\n",
    "    \n",
    "    # Create a map from entity -> index\n",
    "    entity2idx = {ent: i for i, ent in enumerate(entities)}\n",
    "    \n",
    "    # Initialize adjacency matrix\n",
    "    N = len(entities)\n",
    "    adjacency_matrix = np.zeros((N, N), dtype=int)\n",
    "    \n",
    "    # Fill adjacency matrix\n",
    "    for t in triples:\n",
    "        s_idx = entity2idx[t['subject']]\n",
    "        o_idx = entity2idx[t['object']]\n",
    "        adjacency_matrix[s_idx, o_idx] = 1  # or any weighting if needed\n",
    "    \n",
    "    return adjacency_matrix, entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test = 5  # Process only the first 5 entries as a test\n",
    "\n",
    "processed_dataset = []\n",
    "\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= max_test:\n",
    "            break  # stop after a few lines\n",
    "\n",
    "        entry = json.loads(line)\n",
    "        triple_string = entry['triplet']\n",
    "        text_string = entry['text']\n",
    "\n",
    "        # a) Parse triples\n",
    "        parsed = parse_triples(triple_string)\n",
    "\n",
    "        # b) Linearize the triple set\n",
    "        linearized_triples = linearize_triples(parsed)\n",
    "\n",
    "        # c) Tokenize the text & linearized triples (with padding & truncation)\n",
    "        # ---------------------------------------------------------\n",
    "        text_encodings = tokenizer(\n",
    "            text_string,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"  # or \"tf\" / \"np\" as you prefer\n",
    "        )\n",
    "        triple_encodings = tokenizer(\n",
    "            linearized_triples,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Convert input_ids and attention_masks to Python lists\n",
    "        text_input_ids = text_encodings[\"input_ids\"].squeeze(0).tolist()\n",
    "        text_attention_mask = text_encodings[\"attention_mask\"].squeeze(0).tolist()\n",
    "\n",
    "        triple_input_ids = triple_encodings[\"input_ids\"].squeeze(0).tolist()\n",
    "        triple_attention_mask = triple_encodings[\"attention_mask\"].squeeze(0).tolist()\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        # d) Graph structure encoding\n",
    "        edge_list = build_edge_list(parsed)\n",
    "        adjacency_matrix, entities = build_adjacency_matrix(parsed)\n",
    "\n",
    "        # e) Store final outputs\n",
    "        processed_dataset.append({\n",
    "            \"original_text\": text_string,\n",
    "            \"original_triples\": triple_string,\n",
    "            \"parsed_triples\": parsed,\n",
    "            \"linearized_triples_str\": linearized_triples,\n",
    "\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            \"text_attention_mask\": text_attention_mask,\n",
    "            \"triple_input_ids\": triple_input_ids,\n",
    "            \"triple_attention_mask\": triple_attention_mask,\n",
    "\n",
    "            \"edge_list\": edge_list,\n",
    "            \"adjacency_matrix\": adjacency_matrix.tolist(),\n",
    "            \"entities\": entities\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 entries and saved to 'C:\\Users\\Ribhav\\Desktop\\UIUC\\Classes\\Git Repos\\Work\\BGTT\\Dataset\\processed.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "out_file = r\"C:\\Users\\Ribhav\\Desktop\\UIUC\\Classes\\Git Repos\\Work\\BGTT\\Dataset\\processed.jsonl\"\n",
    "with open(out_file, 'w', encoding='utf-8') as f_out:\n",
    "    for item in processed_dataset:\n",
    "        f_out.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Processed {len(processed_dataset)} entries and saved to '{out_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
